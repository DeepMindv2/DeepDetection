{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    }
   ],
   "source": [
    "# Necessary Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import cv2\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.layers import Dense,Conv2D, Flatten, Dropout, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "#%load_ext tensorboard\n",
    "\n",
    "from New_configs import *\n",
    "\n",
    "print('Started')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is based of Stanford Pneumonia Classification algorithm. In their paper they used a 121-layer convolutional neural network called CheXNet. Their network was able to classify 14 thoraric diseases as well as provide a heat map on images to help aid doctors in their diagnosis. \n",
    "- 70% TRAIN\n",
    "- 10% VALIDATION\n",
    "- 20% TEST\n",
    "\n",
    "- Elementwise Sigmoid Nonlinearity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "STEPS_EPOCH = 3\n",
    "LR = 0.001\n",
    "METRICS = ['accuracy','binary_crossentropy', tf.keras.metrics.FalseNegatives(),tf.keras.metrics.FalsePositives(),tf.keras.metrics.Precision(),tf.keras.metrics.Recall()]\n",
    "\n",
    "IMG_SIZE = 160\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_train = '/Users/ryanjoseph/Desktop/Yale/Repositories/Pneumonia/Multi_Chest/Data/train/'\n",
    "data_dir_val = '/Users/ryanjoseph/Desktop/Yale/Repositories/Pneumonia/Multi_Chest/Data/val/'\n",
    "\n",
    "CLASS_NAMES = ['mass','cardiomegaly', 'atelectasis', 'effusion', 'pneumothorax', 'no_finding', 'nodule', 'infiltration', 'consolidation']\n",
    "# 9 Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generators \n",
    "This are functions that do all the image pre-processing and augmentation. From there we send it straight to the Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23777 images belonging to 9 classes.\n",
      "Found 5948 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "validation_image_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data_gen = image_generator.flow_from_directory(directory=str(data_dir_train),\n",
    "                                                     batch_size=BATCH_SIZE,\n",
    "                                                     shuffle=True,\n",
    "                                                     seed=1,\n",
    "                                                     target_size=(160, 160),\n",
    "                                                     classes = list(CLASS_NAMES))\n",
    "\n",
    "val_data_gen = validation_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n",
    "                                                              directory=str(data_dir_val),\n",
    "                                                              seed=1,\n",
    "                                                              target_size=(160, 160),\n",
    "                                                              classes = list(CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.DenseNet121(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "predictions = Dense(9, activation='sigmoid',name='Final')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "tensorboard_logs=\"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_logs, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3/3 [==============================] - 80s 27s/step - loss: 0.2543 - accuracy: 0.9306 - val_loss: 0.4038 - val_accuracy: 0.8463\n",
      "Epoch 2/3\n",
      "3/3 [==============================] - 75s 25s/step - loss: 0.2282 - accuracy: 0.9005 - val_loss: 0.2335 - val_accuracy: 0.9026\n",
      "Epoch 3/3\n",
      "3/3 [==============================] - 74s 25s/step - loss: 0.1356 - accuracy: 0.9352 - val_loss: 0.6374 - val_accuracy: 0.8416\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=STEPS_EPOCH,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data = val_data_gen,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Model \n",
    "model_2 = Sequential([\n",
    "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(160, 160 ,3)),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(9, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3/3 [==============================] - 45s 15s/step - loss: 1.7063 - accuracy: 0.7199 - val_loss: 1.7139 - val_accuracy: 0.8889\n",
      "Epoch 2/3\n",
      "3/3 [==============================] - 47s 16s/step - loss: 1.7139 - accuracy: 0.8889 - val_loss: 1.7139 - val_accuracy: 0.8889\n",
      "Epoch 3/3\n",
      "3/3 [==============================] - 43s 14s/step - loss: 1.7139 - accuracy: 0.8889 - val_loss: 1.7139 - val_accuracy: 0.8889\n"
     ]
    }
   ],
   "source": [
    "history = model_2.fit_generator(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=STEPS_EPOCH,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data = val_data_gen,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
